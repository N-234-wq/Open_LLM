{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387eb2d-159c-4489-b098-82c359e1ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymed import PubMed\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8bec83-b74f-4708-8ad8-7a568b310f6a",
   "metadata": {},
   "source": [
    "# 1. Retrieve article metadata and store it in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c74c3-9e9c-4ee0-9478-abaede5da95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = r'(\"medlinestatus medline\"[All Fields] NOT (\"indexingmethod curated\"[All Fields] OR \"indexingmethod automated\"[All Fields])) AND ((fha[Filter]) AND (classicalarticle[Filter] OR introductoryjournalarticle[Filter]) AND (english[Filter]) AND (2015:2015[pdat]))'\n",
    "\n",
    "pubmed = PubMed(tool=\"HumanIndexingSamples\", email=\"nstrauc3@smail.uni-koeln.de\")\n",
    "results = pubmed.query(query, max_results=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44086ee-7cb1-4069-b0e0-3f747c58229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        field = [\"id\", \"title\", \"publication_date\", \"abstract\", \"MeSH\", \"Year\"]\n",
    "\n",
    "        writer.writerow(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eee15a-0385-4ce4-9bec-9c915e95d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in results:\n",
    "\n",
    "    # Extract relevant info for each article \n",
    "    article_id = article.pubmed_id\n",
    "    title = article.title\n",
    "    publication_date = article.publication_date\n",
    "    abstract = article.abstract\n",
    "\n",
    "    x_path_descriptor = \".//MeshHeading/DescriptorName\"\n",
    "    x_path_completed_date = \".//DateCompleted/Year\"\n",
    "    xml_element = article.xml\n",
    "    mesh_terms = []\n",
    "    \n",
    "    for item in xml_element.findall(x_path_descriptor):\n",
    "        m = re.search(r\"(?<=>).*(?=<)\", ET.tostring(item, encoding=\"unicode\"))\n",
    "        mesh_terms.append(m.group())\n",
    "        \n",
    "    for item in xml_element.findall(x_path_completed_date):\n",
    "        date_completed = re.search(r\"(?<=>).*(?=<)\", ET.tostring(item, encoding=\"unicode\"))\n",
    "\n",
    "    with open('articles.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([article_id, title, publication_date, abstract, \"|\".join(mesh_terms), date_completed.group()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e0346-bb64-4429-83e3-050b2c09c86c",
   "metadata": {},
   "source": [
    "# 2. Prompt LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cc534-0abc-4fdb-8a43-df51ffee57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1e156-8757-45b8-a1ab-ea8fdbde9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords.csv\", \"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"LLM_Keywords\", \"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1177d5-fe02-4a04-a6e0-b2d4b4f4a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"articles.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        title = row[\"title\"]\n",
    "        abstract = row[\"abstract\"]\n",
    "        year = row[\"Year\"]\n",
    "\n",
    "\n",
    "        stream = ollama.chat(\n",
    "            model=\"HammerAI/openhermes-2.5-mistral\",\n",
    "            stream=True,\n",
    "            options={\n",
    "                \"temperature\": 0.0,\n",
    "                \"seed\": 0,\n",
    "            },\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"You are an expert in scientific literature analysis.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Generate a list of keywords that best describe the following document, based on its title and abstract. Output only a numbered list with one keyword per line. The number of keywords should be variable, with an average target of 6–7 keywords, but may be more or fewer as appropriate, and must never exceed 15. Focus on the main concepts, topics, and important ideas presented. Prioritize specific, unique terms that accurately capture the document’s key themes, and avoid generic or overly broad terms.Use only English words.\\nDo not include the names of individuals, organizations, conferences, symposiums, workshops, journals, or events, as these are unsuitable as descriptive keywords.\\nTitle: {title}\\nAbstract: {abstract}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        message = \"\"\n",
    "\n",
    "        for chunk in stream:\n",
    "            if hasattr(chunk[\"message\"], \"thinking\") and chunk[\"message\"].get(\"thinking\"):\n",
    "                print(chunk[\"message\"][\"thinking\"], end=\"\", flush=True)\n",
    "            message += chunk[\"message\"][\"content\"]\n",
    "            #print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "\n",
    "        #print()  # Final newline\n",
    "        with open(\"keywords.csv\", \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([message, year])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31994650-ff4a-4ab7-a2a2-21751ebca96e",
   "metadata": {},
   "source": [
    "# 3. Create embeddings for LLM Keywords + MeSH Terms| Map LLM Keywords to the appropriate MeSH Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862f4b9-cd57-4385-b4ad-72075e7c12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fbb80-5dbb-4fda-b6e6-cb1ae3f8c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(path_to_xml: str) -> list[str]:\n",
    "\n",
    "    tree = ET.parse(path_to_xml) \n",
    "\n",
    "    corpus = []\n",
    "\n",
    "    path = r\".//DescriptorName\"\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for elem in root.findall(path):\n",
    "        corpus.append(elem.find(\"String\").text)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15b0b4-41f4-41a7-8936-32c6c78cf99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_files = {\n",
    "    2015: \"desc2015.xml\",\n",
    "    2016: \"desc2016.xml\",\n",
    "    2017: \"desc2017.xml\",\n",
    "}\n",
    "\n",
    "embedder = SentenceTransformer(\"lokeshch19/ModernPubMedBERT\")\n",
    "\n",
    "mesh_corpora = {}\n",
    "mesh_embeddings = {}\n",
    "\n",
    "for year, path in mesh_files.items():\n",
    "    corpus = create_corpus(path)\n",
    "    mesh_corpora[year] = corpus\n",
    "    mesh_embeddings[year] = embedder.encode_document(\n",
    "        corpus, convert_to_tensor=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c264fb-7169-4f40-bbed-f9b358daebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"mapped_terms.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Original_Keywords\",\"Mapped_Terms\",\"Scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78886735-22ec-4416-b109-56d002eb3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keywords.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        \n",
    "        year = int(row[\"Year\"])\n",
    "        keywords = row[\"LLM_Keywords\"]\n",
    "\n",
    "        corpus = mesh_corpora[year]\n",
    "        corpus_emb = mesh_embeddings[year]\n",
    "\n",
    "        mapped_terms = []\n",
    "        score_terms = []\n",
    "        \n",
    "        if \"1.\" in keywords:\n",
    "            keywords = re.sub(r\"\\d+\\.\\s\", \"\", keywords)\n",
    "        else:\n",
    "            keywords = re.sub(r\",\\s\", \"\\n\", keywords)\n",
    "        \n",
    "        for keyword in keywords.split(\"\\n\"):\n",
    "            #keyword = re.sub(r\"^\\d|\\\"\", \"\", keyword)\n",
    "\n",
    "            query_emb = embedder.encode_query(\n",
    "                keyword, convert_to_tensor=True\n",
    "            )\n",
    "\n",
    "            similarity_scores = embedder.similarity(\n",
    "                query_emb, corpus_emb\n",
    "            )[0]\n",
    "\n",
    "            scores, indices = torch.topk(similarity_scores, k=1)\n",
    "\n",
    "            for score, idx in zip(scores, indices):\n",
    "                mapped_terms.append(corpus[idx])\n",
    "                score_terms.append(f\"{score:.4f}\")\n",
    "                \n",
    "        with open (\"mapped_terms.csv\", \"a\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([keywords, \"\\n\".join(mapped_terms), \"\\n\".join(score_terms)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0d22e-11df-407c-b068-e87eee2c6a4a",
   "metadata": {},
   "source": [
    "## 4. Analyse der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c29dc8-b23f-4726-9ce3-9791999dab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2755bde-477d-455d-9266-1ee4efea616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = {\"MeSH_Keywords\" :[] ,\n",
    "         \"LLM_Keywords\" : []\n",
    "         }\n",
    "\n",
    "\n",
    "with open(\"Ergebnisse.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    for row in reader:\n",
    "    \n",
    "        llm_keywords = row[\"Original_Keywords\"]\n",
    "        mesh_keywords = row[\"MeSH\"]\n",
    "\n",
    "        length[\"MeSH_Keywords\"].append(len(mesh_keywords.split(\"|\")))\n",
    "        length[\"LLM_Keywords\"].append(len(llm_keywords.split(\"\\n\")))\n",
    "        \n",
    "average_mesh = sum(length[\"MeSH_Keywords\"]) / len(length[\"MeSH_Keywords\"])\n",
    "average_llm = sum(length[\"LLM_Keywords\"]) / len(length[\"LLM_Keywords\"])\n",
    "\n",
    "print(f\"On Average Documents were {average_mesh} assigned.\")\n",
    "print(f\"On Average The Open LLM assigned {average_llm} Keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a552235-871c-4c05-b9be-0eb695bc5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = {\n",
    "    \"0<\": 0,\n",
    "    \"0–0.25\": 0,\n",
    "    \"0.26–0.50\": 0,\n",
    "    \"0.51–0.75\": 0,\n",
    "    \"0.76–1.00\": 0\n",
    "}\n",
    "\n",
    "with open(\"Ergebnisse.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        scores = row[\"Scores\"].split(\"\\n\")\n",
    "        \n",
    "        for score in scores:\n",
    "            score = float(score)\n",
    "    \n",
    "            \n",
    "            if 0.0 >= score:\n",
    "                bins[\"0<\"] += 1\n",
    "            elif 0.0 < score <= 0.25:\n",
    "                bins[\"0–0.25\"] += 1\n",
    "            elif 0.26 <= score <= 0.50:\n",
    "                bins[\"0.26–0.50\"] += 1\n",
    "            elif 0.51 <= score <= 0.75:\n",
    "                bins[\"0.51–0.75\"] += 1\n",
    "            elif 0.76 <= score <= 1.00:\n",
    "                bins[\"0.76–1.00\"] += 1\n",
    "\n",
    "\n",
    "labels = list(bins.keys())\n",
    "values = list(bins.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, values, color=\"blue\", edgecolor=\"black\")\n",
    "\n",
    "plt.xlabel(\"Score-Bereich\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.title(\"Verteilung Kosinusähnlichkeit\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('Verteilung_Kosinus.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ab148-9930-407f-964b-0c46cfe194e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union MeSH Keywords and LLM Keywords\n",
    "found = 0\n",
    "# Total MeSH Keywords\n",
    "total_mesh = 0\n",
    "# Total LLM Keywords\n",
    "total_llm_keywords = 0\n",
    "\n",
    "with open(\"Ergebnisse.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        llm_mapped_keywords = row[\"Mapped_Terms\"]\n",
    "        true_keywords = row[\"MeSH\"]\n",
    "\n",
    "        \n",
    "        llm_set = set(keyword.strip().lower() for keyword in llm_mapped_keywords.split(\"\\n\"))\n",
    "        true_set = set(keyword.strip().lower() for keyword in true_keywords.split(\"|\"))\n",
    "\n",
    "\n",
    "        found += len(true_set & llm_set)\n",
    "        total_mesh += len(true_set)\n",
    "        total_llm_keywords += len(llm_set)\n",
    "\n",
    "\n",
    "recall = found / total_mesh \n",
    "precision = found / total_llm_keywords\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"F1-Score: {f1_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc455d-a3ef-49b1-8a76-706e803400e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"LLM Keywords\", \"MeSH Keywords\", \"Richtig\"]\n",
    "values = [total_llm_keywords, total_mesh, found]\n",
    "colors = [\"orange\", \"blue\", \"green\"]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, values, color=colors, edgecolor=\"black\")\n",
    "        \n",
    "legend_patch = patches.Patch(color='white', label=f\"Ø Keywords pro Dokument:\\nMeSH={average_mesh:.1f}, LLM={average_llm:.1f}\")\n",
    "\n",
    "plt.legend(handles=[legend_patch])\n",
    "plt.ylabel(\"Anzahl Keywords\")\n",
    "plt.title(\"Übersicht Keywords\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Übersicht_Keywords.png')\n",
    "plt.show()\n",
    "print(f\"LLM_Keywords: {total_llm_keywords}\")\n",
    "print(f\"MeSH_Keywords: {total_mesh}\")\n",
    "print(f\"Richtig: {found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3633ea-deaf-4535-8ec6-5cc4a7ec4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Recall\", \"Precision\", \"F1-Score\"]\n",
    "values = [recall, precision, f1_score]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(metrics, values, color=[\"yellow\", \"red\", \"black\"], edgecolor=\"black\")\n",
    "plt.ylim(0, 1)  # 0–100%\n",
    "plt.ylabel(\"Wert\")\n",
    "plt.title(\"Keyword-Matching Performance\")\n",
    "\n",
    "# Prozentwerte über die Balken schreiben\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.02, f\"{v:.1%}\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Keywords_Matching.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
